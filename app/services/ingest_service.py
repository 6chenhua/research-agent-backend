"""
è®ºæ–‡æ‘„å…¥æœåŠ¡
è´Ÿè´£è§£æPDFè®ºæ–‡å¹¶å°†å†…å®¹æ·»åŠ åˆ°çŸ¥è¯†å›¾è°±
"""
import logging
import uuid
from datetime import datetime
from typing import Dict, Optional, List
from fastapi import UploadFile, HTTPException
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy import select

from app.core.graphiti_enhanced import enhanced_graphiti
from app.services.pdf_parser import PDFParser
from app.models.db_models import Paper, PaperStatus
from graphiti_core.nodes import EpisodeType

logger = logging.getLogger(__name__)


class IngestService:
    """
    è®ºæ–‡æ‘„å…¥æœåŠ¡ï¼ˆä½¿ç”¨å¢å¼ºç‰ˆ Graphiti å®¢æˆ·ç«¯ï¼‰
    
    å¤„ç†æµç¨‹ï¼š
    1. è§£æPDFæ–‡ä»¶ï¼ˆä½¿ç”¨deepdocï¼‰
    2. æå–å…ƒæ•°æ®å’Œç« èŠ‚
    3. å°†æ¯ä¸ªç« èŠ‚ä½œä¸ºEpisodeæ·»åŠ åˆ°Graphiti
    4. Graphitiè‡ªåŠ¨è¿›è¡Œå®ä½“æŠ½å–å’Œå…³ç³»æ„å»º
    5. ä¿å­˜è®ºæ–‡å…ƒæ•°æ®åˆ°MySQL
    
    ä¼˜åŒ–ç‰¹æ€§ï¼š
    - âœ… ä½¿ç”¨å…¨å±€å•ä¾‹å®¢æˆ·ç«¯ï¼ˆèµ„æºé«˜æ•ˆï¼‰
    - âœ… å¹¶å‘æ§åˆ¶ï¼ˆæ¯ç”¨æˆ·æœ€å¤š2ä¸ªå¹¶å‘ä¸Šä¼ ï¼‰
    - âœ… è¶…æ—¶ä¿æŠ¤ï¼ˆ5åˆ†é’Ÿè‡ªåŠ¨è¶…æ—¶ï¼‰
    - âœ… è¯¦ç»†æ—¥å¿—å’Œç›‘æ§
    """

    def __init__(self, db: AsyncSession = None):
        self.parser = PDFParser()
        self.graph = enhanced_graphiti  # â† ä½¿ç”¨å¢å¼ºç‰ˆå…¨å±€å•ä¾‹
        self.db = db

    async def ingest_pdf(
        self, 
        file: UploadFile, 
        user_id: str,
        group_id: Optional[str] = None
    ) -> Dict:
        """
        æ‘„å…¥PDFè®ºæ–‡åˆ°çŸ¥è¯†å›¾è°±
        
        Args:
            file: ä¸Šä¼ çš„PDFæ–‡ä»¶
            user_id: ç”¨æˆ·ID
            group_id: å›¾è°±å‘½åç©ºé—´IDï¼Œé»˜è®¤ä¸ºç”¨æˆ·å‘½åç©ºé—´
            
        Returns:
            åŒ…å«paper_id, title, statusç­‰ä¿¡æ¯çš„å­—å…¸
        """
        # å‚æ•°éªŒè¯
        if not file.filename.endswith('.pdf'):
            raise HTTPException(status_code=400, detail="Only PDF files are supported")
        
        # æ–‡ä»¶å¤§å°é™åˆ¶ (50MB)
        max_size = 50 * 1024 * 1024
        file_bytes = await file.read()
        if len(file_bytes) > max_size:
            raise HTTPException(
                status_code=400, 
                detail=f"File size exceeds 50MB limit"
            )
        
        try:
            # Step 1: è§£æPDF
            logger.info(f"Parsing PDF: {file.filename}")
            parsed_data = await self.parser.parse(file_bytes, file.filename)
            
            # Step 2: ç”Ÿæˆpaper_id
            paper_id = f"paper_{uuid.uuid4().hex[:12]}"
            
            # Step 3: è®¾ç½®å‘½åç©ºé—´ï¼ˆç”¨æˆ·å›¾è°±ï¼‰
            if not group_id:
                group_id = f"user:{user_id}"
            
            # Step 4: å°†ç« èŠ‚å†…å®¹ä½œä¸ºEpisodesæ·»åŠ åˆ°Graphitiï¼ˆå¹¶å‘ä¼˜åŒ–ï¼‰
            logger.info(f"Adding {len(parsed_data['sections'])} sections to graph for paper: {parsed_data['title']}")
            
            # å¹¶å‘æ·»åŠ episodesï¼ˆæå‡æ€§èƒ½ï¼‰
            episode_results = await self._add_episodes_concurrent(
                parsed_data=parsed_data,
                paper_id=paper_id,
                user_id=user_id,
                group_id=group_id
            )
            
            # Step 5: ä¿å­˜è®ºæ–‡å…ƒæ•°æ®åˆ°MySQL
            if self.db:
                await self._save_paper_metadata(
                    paper_id=paper_id,
                    parsed_data=parsed_data,
                    file_name=file.filename
                )
            
            logger.info(f"Successfully ingested paper: {paper_id}")
            
            return {
                "paper_id": paper_id,
                "title": parsed_data['title'],
                "authors": parsed_data.get('authors', []),
                "year": parsed_data.get('year'),
                "sections_count": len(parsed_data['sections']),
                "episodes_added": len(episode_results),
                "status": "success",
                "group_id": group_id
            }
            
        except Exception as e:
            logger.error(f"Ingestion failed for {file.filename}: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to ingest PDF: {str(e)}"
            )

    async def _add_episodes_concurrent(
        self,
        parsed_data: Dict,
        paper_id: str,
        user_id: str,
        group_id: str,
        max_concurrent: int = 3
    ) -> List:
        """
        å¹¶å‘æ·»åŠ å¤šä¸ªepisodesåˆ°Graphiti
        
        ä¼˜åŒ–è¯´æ˜ï¼š
        - ä½¿ç”¨asyncio.Semaphoreæ§åˆ¶å¹¶å‘æ•°é‡
        - é¿å…åŒæ—¶å‘èµ·è¿‡å¤šè¯·æ±‚å‹å®Graphiti/Neo4j
        - ä¿æŒé”™è¯¯å¤„ç†ï¼Œå¤±è´¥çš„episodeä¸å½±å“å…¶ä»–episode
        
        Args:
            parsed_data: è§£æåçš„è®ºæ–‡æ•°æ®
            paper_id: è®ºæ–‡ID
            user_id: ç”¨æˆ·ID
            group_id: å›¾è°±å‘½åç©ºé—´
            max_concurrent: æœ€å¤§å¹¶å‘æ•°ï¼ˆé»˜è®¤3ï¼Œå¯æ ¹æ®æœåŠ¡å™¨æ€§èƒ½è°ƒæ•´ï¼‰
            
        Returns:
            æˆåŠŸæ·»åŠ çš„episodeç»“æœåˆ—è¡¨
        """
        import asyncio
        
        sections = parsed_data['sections']
        semaphore = asyncio.Semaphore(max_concurrent)
        episode_results = []
        
        async def add_single_episode(idx: int, section: Dict):
            """æ·»åŠ å•ä¸ªepisodeï¼ˆå¸¦å¹¶å‘æ§åˆ¶ï¼‰"""
            async with semaphore:  # æ§åˆ¶å¹¶å‘æ•°é‡
                try:
                    # æ„å»ºEpisodeå†…å®¹
                    episode_content = self._build_episode_content(
                        paper_id=paper_id,
                        title=parsed_data['title'],
                        section=section,
                        section_idx=idx
                    )
                    
                    logger.info(
                        f"  [{idx+1}/{len(sections)}] Adding section: "
                        f"{section.get('heading', 'N/A')[:30]}... "
                        f"(content: {len(episode_content)} chars)"
                    )
                    
                    # è°ƒç”¨Graphiti.add_episode
                    result = await self.graph.add_episode(
                        episode_body=episode_content,
                        user_id=user_id,
                        group_id=group_id,
                        name=f"{paper_id}_section_{idx+1}",
                        source=EpisodeType.text,
                        source_description=f"Section {idx+1} from paper: {parsed_data['title']}",
                        reference_time=datetime.utcnow(),
                        timeout=300.0
                    )
                    
                    logger.info(f"  âœ… [{idx+1}/{len(sections)}] Section added successfully")
                    return result
                    
                except Exception as e:
                    logger.error(
                        f"  âŒ [{idx+1}/{len(sections)}] Failed to add section: {str(e)}",
                        exc_info=True
                    )
                    return None  # è¿”å›Noneè¡¨ç¤ºå¤±è´¥
        
        # åˆ›å»ºæ‰€æœ‰ä»»åŠ¡
        tasks = [
            add_single_episode(idx, section)
            for idx, section in enumerate(sections)
        ]
        
        # å¹¶å‘æ‰§è¡Œæ‰€æœ‰ä»»åŠ¡
        logger.info(f"ğŸš€ Starting concurrent upload with max_concurrent={max_concurrent}")
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # è¿‡æ»¤æˆåŠŸçš„ç»“æœ
        episode_results = [r for r in results if r is not None and not isinstance(r, Exception)]
        
        success_count = len(episode_results)
        total_count = len(sections)
        logger.info(
            f"ğŸ“Š Episode upload complete: {success_count}/{total_count} succeeded, "
            f"{total_count - success_count} failed"
        )
        
        return episode_results
    
    def _build_episode_content(
        self,
        paper_id: str,
        title: str,
        section: Dict,
        section_idx: int
    ) -> str:
        """
        æ„å»ºEpisodeå†…å®¹
        
        å°†ç« èŠ‚å†…å®¹æ ¼å¼åŒ–ä¸ºé€‚åˆGraphitiå¤„ç†çš„æ–‡æœ¬
        """
        heading = section.get('heading', f'Section {section_idx + 1}')
        content = section.get('content', '')
        
        # æ„å»ºç»“æ„åŒ–çš„Episodeå†…å®¹
        episode_text = f"""
Paper Title: {title}
Paper ID: {paper_id}
Section: {heading}

{content}
""".strip()
        
        return episode_text

    async def _save_paper_metadata(
        self, 
        paper_id: str, 
        parsed_data: Dict,
        file_name: str
    ):
        """
        æ›´æ–°è®ºæ–‡è§£æå†…å®¹åˆ°MySQL
        """
        try:
            result = await self.db.execute(
                select(Paper).filter(Paper.id == paper_id)
            )
            paper = result.scalar_one_or_none()
            
            if paper:
                paper.parsed_content = parsed_data
                paper.status = PaperStatus.PARSED
                paper.parsed_at = datetime.utcnow()
                await self.db.commit()
                logger.info(f"Updated paper parsed content: {paper_id}")
            else:
                logger.warning(f"Paper not found for metadata update: {paper_id}")
            
        except Exception as e:
            logger.error(f"Failed to save metadata: {str(e)}")
            await self.db.rollback()
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå› ä¸ºä¸»è¦é€»è¾‘ï¼ˆå›¾è°±æ‘„å…¥ï¼‰å·²å®Œæˆ

    async def get_paper_detail(
        self, 
        paper_id: str,
        user_id: str,
        group_id: Optional[str] = None
    ) -> Dict:
        """
        è·å–è®ºæ–‡è¯¦æƒ…
        
        åŒ…å«ï¼š
        1. MySQLä¸­çš„å…ƒæ•°æ®
        2. å›¾è°±ä¸­çš„å®ä½“å’Œå…³ç³»
        3. ç›¸å…³è®ºæ–‡æ¨è
        
        Args:
            paper_id: è®ºæ–‡ID
            user_id: ç”¨æˆ·ID
            group_id: å‘½åç©ºé—´
            
        Returns:
            è®ºæ–‡è¯¦æƒ…å­—å…¸
        """
        if not group_id:
            group_id = f"user:{user_id}"
        
        # Step 1: ä»MySQLè·å–è®ºæ–‡ä¿¡æ¯
        paper = None
        if self.db:
            result = await self.db.execute(
                select(Paper).filter(Paper.id == paper_id)
            )
            paper = result.scalar_one_or_none()
        
        if not paper:
            raise HTTPException(status_code=404, detail="Paper not found")
        
        # ä»parsed_contentä¸­æå–å…ƒæ•°æ®
        parsed_content = paper.parsed_content or {}
        title = parsed_content.get('title', paper.filename)
        
        # Step 2: ä»å›¾è°±è·å–ç›¸å…³å®ä½“
        try:
            # æœç´¢ä¸è®ºæ–‡ç›¸å…³çš„èŠ‚ç‚¹
            search_results = await self.graph.search(
                query=title,
                group_id=group_id,
                limit=20
            )
            
            # æå–å®ä½“
            entities = self._extract_entities_from_search(search_results)
            
            # Step 3: æ¨èç›¸å…³è®ºæ–‡ï¼ˆåŸºäºå›¾è°±æœç´¢ï¼‰
            related_papers = await self._find_related_papers(
                paper_id=paper_id,
                group_id=group_id,
                limit=5
            )
            
            return {
                "paper_id": paper_id,
                "title": title,
                "authors": parsed_content.get('authors', []),
                "abstract": parsed_content.get('abstract', ''),
                "year": parsed_content.get('metadata', {}).get('publication_year'),
                "venue": parsed_content.get('metadata', {}).get('conference'),
                "filename": paper.filename,
                "domain": paper.domain,
                "status": paper.status.value if paper.status else None,
                "added_to_graph": paper.added_to_graph,
                "entities": entities,
                "related_papers": related_papers,
                "created_at": paper.created_at.isoformat() if paper.created_at else None
            }
            
        except Exception as e:
            logger.error(f"Failed to get paper detail: {str(e)}")
            raise HTTPException(
                status_code=500,
                detail=f"Failed to retrieve paper details: {str(e)}"
            )

    def _extract_entities_from_search(self, search_results: list) -> list:
        """
        ä»æœç´¢ç»“æœä¸­æå–å®ä½“ä¿¡æ¯
        """
        entities = []
        for result in search_results[:10]:  # é™åˆ¶è¿”å›æ•°é‡
            if hasattr(result, 'node') and result.node:
                node = result.node
                entities.append({
                    "uuid": getattr(node, 'uuid', ''),
                    "name": getattr(node, 'name', ''),
                    "type": getattr(node, 'labels', ['Unknown'])[0] if hasattr(node, 'labels') else 'Unknown',
                    "summary": getattr(node, 'summary', '')
                })
        return entities

    async def _find_related_papers(
        self, 
        paper_id: str, 
        group_id: str, 
        limit: int = 5
    ) -> list:
        """
        åŸºäºå›¾è°±æŸ¥æ‰¾ç›¸å…³è®ºæ–‡
        """
        try:
            # ä½¿ç”¨è®ºæ–‡IDä½œä¸ºæŸ¥è¯¢ï¼Œæ‰¾åˆ°ç›¸å…³èŠ‚ç‚¹
            results = await self.graph.search(
                query=paper_id,
                group_id=group_id,
                limit=limit * 2  # å¤šæ‹¿ä¸€äº›ï¼Œè¿‡æ»¤åå†è¿”å›
            )
            
            related = []
            for result in results:
                if hasattr(result, 'node') and result.node:
                    node = result.node
                    # å¦‚æœæ˜¯Paperç±»å‹çš„èŠ‚ç‚¹
                    if 'Paper' in getattr(node, 'labels', []):
                        related.append({
                            "paper_id": getattr(node, 'uuid', ''),
                            "title": getattr(node, 'name', ''),
                            "relevance_score": getattr(result, 'score', 0.0)
                        })
            
            return related[:limit]
            
        except Exception as e:
            logger.warning(f"Failed to find related papers: {str(e)}")
            return []