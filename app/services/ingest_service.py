"""
è®ºæ–‡æ‘„å…¥æœåŠ¡
è´Ÿè´£è§£æPDFè®ºæ–‡å¹¶å°†å†…å®¹æ·»åŠ åˆ°çŸ¥è¯†å›¾è°±

ä½¿ç”¨ç»Ÿä¸€çš„å®ä½“ç±»å‹å’Œå…³ç³»ç±»å‹ï¼š
- å®ä½“ç±»å‹ï¼šPaper, Method, Dataset, Task, Metric, Author, Institution, Concept
- å…³ç³»ç±»å‹ï¼šPROPOSES, EVALUATES_ON, SOLVES, IMPROVES_OVER, CITES, ç­‰
- ç›´æ¥ä½¿ç”¨ entities.py å’Œ relations.py ä¸­çš„è§„èŒƒå®šä¹‰
"""
import logging
import uuid
from datetime import datetime
from typing import Dict, Optional, List
from fastapi import UploadFile, HTTPException
import os

from app.core.graphiti_enhanced import enhanced_graphiti
from app.services.pdf_parser import PDFParser
from app.crud.paper import PaperRepository
from graphiti_core.nodes import EpisodeType
from app.utils.entity_types import get_entity_types, get_relation_types
from app.core.config import settings
from app.models.db_models import Paper, PaperStatus

logger = logging.getLogger(__name__)


class IngestService:
    """
    è®ºæ–‡æ‘„å…¥æœåŠ¡ï¼ˆä½¿ç”¨å¢å¼ºç‰ˆ Graphiti å®¢æˆ·ç«¯ï¼‰
    
    å¤„ç†æµç¨‹ï¼š
    1. è§£æPDFæ–‡ä»¶ï¼ˆä½¿ç”¨deepdocï¼‰
    2. æå–å…ƒæ•°æ®å’Œç« èŠ‚
    3. å°†æ¯ä¸ªç« èŠ‚ä½œä¸ºEpisodeæ·»åŠ åˆ°Graphiti
    4. Graphitiè‡ªåŠ¨è¿›è¡Œå®ä½“æŠ½å–å’Œå…³ç³»æ„å»º
    5. ä¿å­˜è®ºæ–‡å…ƒæ•°æ®åˆ°MySQL
    
    ä¼˜åŒ–ç‰¹æ€§ï¼š
    - âœ… ä½¿ç”¨å…¨å±€å•ä¾‹å®¢æˆ·ç«¯ï¼ˆèµ„æºé«˜æ•ˆï¼‰
    - âœ… å¹¶å‘æ§åˆ¶ï¼ˆæ¯ç”¨æˆ·æœ€å¤š2ä¸ªå¹¶å‘ä¸Šä¼ ï¼‰
    - âœ… è¶…æ—¶ä¿æŠ¤ï¼ˆ5åˆ†é’Ÿè‡ªåŠ¨è¶…æ—¶ï¼‰
    - âœ… è¯¦ç»†æ—¥å¿—å’Œç›‘æ§
    - âœ… ä½¿ç”¨ Repository Pattern
    """

    def __init__(self, paper_repo: Optional[PaperRepository] = None):
        """
        åˆå§‹åŒ–è®ºæ–‡æ‘„å…¥æœåŠ¡
        
        Args:
            paper_repo: è®ºæ–‡æ•°æ®è®¿é—®å±‚ï¼ˆå¯é€‰ï¼‰
        """
        self.parser = PDFParser()
        self.graph = enhanced_graphiti  # â† ä½¿ç”¨å¢å¼ºç‰ˆå…¨å±€å•ä¾‹
        self.paper_repo = paper_repo

    async def upload_paper(
        self,
        file: UploadFile,
        user_id: str
    ) -> Dict:
        """
        ä¸Šä¼ è®ºæ–‡PDFï¼ˆåªä¿å­˜ï¼Œä¸è§£æï¼‰
        
        Args:
            file: ä¸Šä¼ çš„PDFæ–‡ä»¶
            user_id: ç”¨æˆ·ID
            
        Returns:
            åŒ…å« paper_id, filename, file_size, status çš„å­—å…¸
            
        Raises:
            HTTPException: æ–‡ä»¶æ ¼å¼æˆ–å¤§å°æ— æ•ˆ
        """

        
        # éªŒè¯æ–‡ä»¶æ ¼å¼
        if not file.filename or not file.filename.lower().endswith('.pdf'):
            raise HTTPException(status_code=400, detail="Only PDF files are supported")
        
        # è¯»å–æ–‡ä»¶å¹¶éªŒè¯å¤§å°
        file_bytes = await file.read()
        max_size = 50 * 1024 * 1024  # 50MB
        
        if len(file_bytes) > max_size:
            raise HTTPException(
                status_code=400,
                detail=f"File size ({len(file_bytes) / 1024 / 1024:.1f}MB) exceeds 50MB limit"
            )
        
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶åå’Œä¿å­˜è·¯å¾„
        paper_id = f"paper_{uuid.uuid4().hex[:12]}"
        safe_filename = f"{paper_id}_{file.filename}"
        
        # ç¡®ä¿ä¸Šä¼ ç›®å½•å­˜åœ¨
        upload_dir = settings.UPLOAD_DIR
        os.makedirs(upload_dir, exist_ok=True)
        
        file_path = os.path.join(upload_dir, safe_filename)
        
        # ä¿å­˜æ–‡ä»¶åˆ°ç£ç›˜
        with open(file_path, 'wb') as f:
            f.write(file_bytes)
        
        logger.info(f"ğŸ“„ File saved: {file_path}")
        
        # åˆ›å»ºæ•°æ®åº“è®°å½•
        if not self.paper_repo:
            raise HTTPException(status_code=500, detail="Paper repository not available")
        
        paper = Paper(
            id=paper_id,
            user_id=user_id,
            filename=file.filename,
            file_path=file_path,
            file_size=len(file_bytes),
            status=PaperStatus.UPLOADED,
            domains=None,
            parsed_content=None,
            created_at=datetime.utcnow()
        )
        
        await self.paper_repo.create(paper)
        
        logger.info(f"âœ… Paper uploaded: {paper_id} ({file.filename})")
        
        return {
            "paper_id": paper_id,
            "filename": file.filename,
            "file_size": len(file_bytes),
            "status": "uploaded",
            "message": "Paper uploaded successfully. It will be parsed when used in chat."
        }
    
    def _build_episode_content(
        self,
        paper_id: str,
        title: str,
        section: Dict,
        section_idx: int,
        domain: str = "General"
    ) -> str:
        """
        æ„å»ºEpisodeå†…å®¹
        
        å°†ç« èŠ‚å†…å®¹æ ¼å¼åŒ–ä¸ºé€‚åˆGraphitiå¤„ç†çš„æ–‡æœ¬ã€‚
        
        è®¾è®¡åŸåˆ™ï¼š
        1. ä¸æŒ‰ section ç±»å‹åŒºåˆ†å®ä½“ç±»å‹ï¼Œè®© LLM æ ¹æ®å†…å®¹è‡ªåŠ¨åˆ¤æ–­
        2. Section æ ‡é¢˜ä½œä¸ºä¸Šä¸‹æ–‡æç¤ºï¼Œå¸®åŠ© LLM ç†è§£å†…å®¹æ€§è´¨
        3. åŒ…å« domain ä¿¡æ¯ä»¥å¸®åŠ© LLM æ›´å‡†ç¡®åœ°æå–é¢†åŸŸå®ä½“
        
        Args:
            paper_id: è®ºæ–‡ID
            title: è®ºæ–‡æ ‡é¢˜
            section: ç« èŠ‚æ•°æ®
            section_idx: ç« èŠ‚ç´¢å¼•
            domain: è®ºæ–‡æ‰€å±é¢†åŸŸ
            
        Returns:
            æ ¼å¼åŒ–çš„ episode å†…å®¹
        """
        heading = section.get('heading', f'Section {section_idx + 1}')
        content = section.get('content', '')
        
        # æ ‡å‡†åŒ– section ç±»å‹æè¿°ï¼ˆå¸®åŠ© LLM ç†è§£ä¸Šä¸‹æ–‡ï¼‰
        section_context = self._get_section_context_hint(heading)
        
        # æ„å»ºç»“æ„åŒ–çš„Episodeå†…å®¹
        # è¿™ä¸ªæ ¼å¼è®¾è®¡æ˜¯ä¸ºäº†è®© Graphiti çš„ LLM æ›´å¥½åœ°ç†è§£ä¸Šä¸‹æ–‡
        episode_text = f"""
[Research Paper Context]
Domain: {domain}
Paper: {title}
Section: {heading}
{section_context}

[Content]
{content}
""".strip()
        
        return episode_text
    
    def _get_section_context_hint(self, heading: str) -> str:
        """
        æ ¹æ® section æ ‡é¢˜ç”Ÿæˆä¸Šä¸‹æ–‡æç¤º
        
        è¿™ä¸æ˜¯ç”¨æ¥åŒºåˆ†å®ä½“ç±»å‹çš„ï¼Œè€Œæ˜¯ç»™ LLM ä¸€ä¸ªæç¤ºï¼Œ
        å¸®åŠ©å®ƒç†è§£å½“å‰å†…å®¹çš„æ€§è´¨ã€‚
        
        Args:
            heading: section æ ‡é¢˜
            
        Returns:
            ä¸Šä¸‹æ–‡æç¤ºå­—ç¬¦ä¸²
        """
        heading_lower = heading.lower()
        
        # å®šä¹‰ section ç±»å‹å’Œå¯¹åº”çš„ä¸Šä¸‹æ–‡æç¤º
        section_hints = {
            # æ‘˜è¦ç±»
            ("abstract",): "This section provides a high-level summary of the paper's contributions and findings.",
            
            # å¼•è¨€ç±»
            ("introduction", "intro"): "This section introduces the problem, motivation, and overview of the approach.",
            
            # ç›¸å…³å·¥ä½œç±»
            ("related work", "background", "literature", "prior work", "previous work"): 
                "This section discusses existing methods and compares them to the proposed approach.",
            
            # æ–¹æ³•ç±»
            ("method", "approach", "methodology", "proposed", "framework", "architecture", "model", "algorithm"):
                "This section describes the proposed method, model, or algorithm in detail.",
            
            # å®éªŒç±»
            ("experiment", "evaluation", "result", "empirical", "analysis"):
                "This section presents experimental setup, results, and analysis.",
            
            # è®¨è®ºç±»
            ("discussion", "limitation", "future work", "conclusion"):
                "This section discusses findings, limitations, and future directions.",
            
            # å®ç°ç±»
            ("implementation", "setup", "configuration", "training"):
                "This section describes implementation details and experimental setup.",
        }
        
        # åŒ¹é… section ç±»å‹
        for keywords, hint in section_hints.items():
            if any(kw in heading_lower for kw in keywords):
                return f"Context: {hint}"
        
        # é»˜è®¤æç¤º
        return "Context: General content from the paper."

    async def add_paper_to_graph(
        self,
        paper_id: str,
        user_id: str
    ) -> Dict:
        """
        å°†å·²è§£æçš„è®ºæ–‡æ·»åŠ åˆ°çŸ¥è¯†å›¾è°±
        
        è¿™æ˜¯ç”¨æˆ·è§¦å‘çš„æ“ä½œï¼Œæµç¨‹ï¼š
        1. ä»æ•°æ®åº“è·å–è®ºæ–‡ä¿¡æ¯ï¼ˆå¿…é¡»å·²è§£æï¼‰
        2. ä½¿ç”¨ LLM åˆ†æ abstract è¯†åˆ« domains
        3. ä¸ºæ¯ä¸ª domain æ„å»º group_id å¹¶æ·»åŠ åˆ°å›¾è°±
        4. ä½¿ç”¨ç»Ÿä¸€çš„å®ä½“ç±»å‹å’Œå…³ç³»ç±»å‹
        5. æ›´æ–°æ•°æ®åº“çŠ¶æ€
        
        Args:
            paper_id: è®ºæ–‡ID
            user_id: ç”¨æˆ·ID
            
        Returns:
            åŒ…å« domains, episodes_added, status ç­‰ä¿¡æ¯çš„å­—å…¸
            
        Raises:
            HTTPException: è®ºæ–‡ä¸å­˜åœ¨æˆ–æœªè§£æ
        """
        from app.services.domain_analyzer import DomainAnalyzer
        from app.utils.group_id import get_paper_ingest_group_ids
        
        # Step 1: è·å–è®ºæ–‡ä¿¡æ¯
        if not self.paper_repo:
            raise HTTPException(status_code=500, detail="Paper repository not available")
        
        paper = await self.paper_repo.get_by_id(paper_id)
        
        if not paper:
            raise HTTPException(status_code=404, detail="Paper not found")
        
        if paper.user_id != user_id:
            raise HTTPException(status_code=403, detail="Access denied")
        
        if paper.status != PaperStatus.PARSED:
            raise HTTPException(
                status_code=400, 
                detail=f"Paper must be parsed first. Current status: {paper.status}"
            )
        
        if paper.added_to_graph:
            raise HTTPException(
                status_code=400, 
                detail="Paper already added to graph"
            )
        
        # Step 2: è·å–è§£æå†…å®¹
        parsed_content = paper.parsed_content
        if not parsed_content:
            raise HTTPException(status_code=400, detail="Paper has no parsed content")
        
        abstract = parsed_content.get('abstract', '')
        title = parsed_content.get('title', paper.filename)
        sections = parsed_content.get('sections', [])
        
        if not abstract and not sections:
            raise HTTPException(status_code=400, detail="Paper has no content to add")
        
        # Step 3: ä½¿ç”¨ LLM åˆ†æ domains
        logger.info(f"Analyzing domains for paper: {paper_id}")
        domain_analyzer = DomainAnalyzer()
        domains = await domain_analyzer.analyze_domains(abstract, title)
        
        logger.info(f"Identified domains: {domains}")
        
        # Step 4: è·å–ç»Ÿä¸€çš„å®ä½“ç±»å‹å’Œå…³ç³»ç±»å‹
        # ç›´æ¥ä½¿ç”¨ entities.py å’Œ relations.py ä¸­çš„è§„èŒƒå®šä¹‰
        entity_types = get_entity_types()
        relation_types = get_relation_types()
        
        # Step 5: æ·»åŠ åˆ°å…¬å…±é¢†åŸŸå›¾è°±
        # æ‰€æœ‰è®ºæ–‡è¿›å…¥å…¬å…±å›¾è°±ï¼ˆdomain:{domain}ï¼‰ï¼Œå®ç°çŸ¥è¯†å…±äº«
        all_episode_ids = []
        group_ids = get_paper_ingest_group_ids(domains)
        
        logger.info(f"Adding paper to public graph: group_ids={group_ids}")
        
        for group_id in group_ids:
            domain = group_id.replace("domain:", "").upper()
            
            # æ·»åŠ æ¯ä¸ª section
            for idx, section in enumerate(sections):
                try:
                    episode_content = self._build_episode_content(
                        paper_id=paper_id,
                        title=title,
                        section=section,
                        section_idx=idx,
                        domain=domain
                    )
                    
                    result = await self.graph.add_episode(
                        episode_body=episode_content,
                        user_id=user_id,  # è®°å½•ä¸Šä¼ è€…ï¼Œä½†æ•°æ®è¿›å…¥å…¬å…±å›¾è°±
                        group_id=group_id,
                        name=f"{paper_id}_{domain}_section_{idx+1}",
                        source=EpisodeType.text,
                        source_description=f"[{domain}] {title}",
                        reference_time=datetime.utcnow(),
                        entity_types=entity_types,
                        edge_types=relation_types,
                        timeout=300.0
                    )
                    
                    if result:
                        all_episode_ids.append(str(result) if result else f"{paper_id}_{domain}_{idx}")
                        
                except Exception as e:
                    logger.error(f"Failed to add section {idx} for domain {domain}: {e}")
                    # ç»§ç»­å¤„ç†å…¶ä»– section
        
        # Step 6: æ›´æ–°æ•°æ®åº“çŠ¶æ€
        try:
            paper.added_to_graph = True
            paper.domains = domains
            paper.graph_episode_ids = all_episode_ids
            paper.added_to_graph_at = datetime.utcnow()
            
            await self.paper_repo.update(paper)
            
            logger.info(f"âœ… Paper {paper_id} added to graph with domains: {domains}")
            
        except Exception as e:
            logger.error(f"Failed to update paper status: {e}")
        
        return {
            "paper_id": paper_id,
            "title": title,
            "domains": domains,
            "sections_count": len(sections),
            "episodes_added": len(all_episode_ids),
            "added_to_graph": True,
            "status": "success"
        }
