"""
èŠå¤©æœåŠ¡
æ ¹æ®PRD_ç ”ç©¶ä¸èŠå¤©æ¨¡å—.mdè®¾è®¡
æä¾›æ¶ˆæ¯å‘é€ã€å†å²è®°å½•æŸ¥è¯¢ç­‰åŠŸèƒ½

å›¾è°±æ¶æ„ï¼š
- å…¬å…±é¢†åŸŸå›¾è°±ï¼šdomain:{domain}ï¼ˆæ‰€æœ‰ç”¨æˆ·å…±äº«çš„è®ºæ–‡çŸ¥è¯†ï¼‰
- ç”¨æˆ·ç§æœ‰ç¬”è®°ï¼šuser:{user_id}:notesï¼ˆç”¨æˆ·ä¸»åŠ¨æ·»åŠ çš„æ¶ˆæ¯/ç¬”è®°ï¼‰
"""
import asyncio
import json
import logging
import time
from datetime import datetime, timezone
from typing import Optional, List, Dict, Any
from uuid import uuid4

from app.models.db_models import MessageRole
from app.core.graphiti_enhanced import get_enhanced_graphiti
from app.integrations.llm_client import LLMClient
from app.crud.session import SessionRepository
from app.crud.message import MessageRepository
from app.crud.paper import PaperRepository
from app.utils.group_id import get_search_group_ids
from app.services.profile_service import ProfileService

logger = logging.getLogger(__name__)


class ChatService:
    """
    èŠå¤©æœåŠ¡
    
    ä½¿ç”¨ Repository Patternï¼Œé€šè¿‡æ„é€ å‡½æ•°æ³¨å…¥æ‰€éœ€çš„ Repository
    
    åŠŸèƒ½ï¼š
    - å‘é€æ¶ˆæ¯å¹¶è·å– AI å›å¤
    - æ ¹æ® session domains ä»å…¬å…±å›¾è°±æ£€ç´¢ç›¸å…³çŸ¥è¯†
    - è§£æè®ºæ–‡å¹¶ç”¨ LLM æå–ç›¸å…³å†…å®¹ä½œä¸ºä¸Šä¸‹æ–‡
    - ç”¨æˆ·ç”»åƒä¸ªæ€§åŒ–ï¼ˆè°ƒæ•´å›å¤é£æ ¼ï¼‰
    
    æ³¨æ„ï¼šæ¶ˆæ¯ä¸ä¼šè‡ªåŠ¨æ·»åŠ åˆ°å›¾è°±ï¼Œç”¨æˆ·å¯é€šè¿‡ add-to-notes ä¸»åŠ¨æ·»åŠ ã€‚
    """

    def __init__(
        self,
        session_repo: SessionRepository,
        message_repo: MessageRepository,
        paper_repo: PaperRepository,
        profile_service: Optional[ProfileService] = None
    ):
        """
        åˆå§‹åŒ–èŠå¤©æœåŠ¡
        
        Args:
            session_repo: ä¼šè¯æ•°æ®è®¿é—®å±‚
            message_repo: æ¶ˆæ¯æ•°æ®è®¿é—®å±‚
            paper_repo: è®ºæ–‡æ•°æ®è®¿é—®å±‚
            profile_service: ç”¨æˆ·ç”»åƒæœåŠ¡ï¼ˆç”¨äºä¸ªæ€§åŒ–ï¼‰
        """
        self.session_repo = session_repo
        self.message_repo = message_repo
        self.paper_repo = paper_repo
        self.profile_service = profile_service
        self.llm_client = LLMClient()

    async def send_message(
        self,
        session_id: str,
        message: str,
        user_id: str,
        attached_papers: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        å‘é€æ¶ˆæ¯ - REQ-CHAT-3
        æ ¸å¿ƒå¤„ç†æµç¨‹ï¼š
        1. éªŒè¯session
        2. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
        3. å¼‚æ­¥æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å›¾è°±
        4. ç”Ÿæˆcontextï¼ˆæ¥è‡ªå›¾è°±æˆ–è®ºæ–‡ï¼‰
        5. LLMç”Ÿæˆå›å¤
        6. ä¿å­˜Agentæ¶ˆæ¯
        7. è¿”å›å“åº”
        
        Args:
            session_id: ä¼šè¯ID
            message: ç”¨æˆ·æ¶ˆæ¯
            user_id: ç”¨æˆ·ID
            attached_papers: é™„å¸¦çš„è®ºæ–‡IDåˆ—è¡¨
            
        Returns:
            åŒ…å«ç”¨æˆ·æ¶ˆæ¯ã€Agentæ¶ˆæ¯å’ŒçŠ¶æ€çš„å“åº”
        """
        attached_papers = attached_papers or []
        
        # 1. éªŒè¯sessionå­˜åœ¨ä¸”å±äºè¯¥ç”¨æˆ·
        research_session = await self.session_repo.get_by_id_and_user(session_id, user_id)
        if not research_session:
            raise ValueError("SESSION_NOT_FOUND")
        
        # 2. è·å–ä¼šè¯çš„domains
        domains = SessionRepository.parse_domains(research_session.domains)
        
        now = datetime.now(timezone.utc)
        
        # 3. ä¿å­˜ç”¨æˆ·æ¶ˆæ¯åˆ°MySQL
        user_msg_id = str(uuid4())
        await self.message_repo.create_message(
            message_id=user_msg_id,
            session_id=session_id,
            role=MessageRole.USER,
            content=message,
            attached_papers=attached_papers if attached_papers else None,
            created_at=now
        )
        
        # 4. ç”Ÿæˆcontext
        if attached_papers:
            # åˆ†æ”¯Aï¼šè®ºæ–‡contextï¼ˆè§£æè®ºæ–‡å¹¶æå–ç›¸å…³å†…å®¹ï¼‰
            context_string, context_data = await self._generate_paper_context(
                attached_papers, message, user_id
            )
        else:
            # åˆ†æ”¯Bï¼šå›¾è°±context
            context_string, context_data = await self._generate_graph_context(
                user_id, message, domains
            )
        
        # 5. è·å–æœ€è¿‘çš„å†å²æ¶ˆæ¯
        recent_messages = await self.message_repo.get_recent(session_id, limit=10)
        history = MessageRepository.to_history_format(recent_messages)
        
        # 6. è·å–ç”¨æˆ·ç”»åƒï¼ˆç”¨äºä¸ªæ€§åŒ–ï¼‰
        user_profile = None
        if self.profile_service:
            user_profile = await self.profile_service.get_user_profile(user_id)
        
        # 7. LLMç”Ÿæˆå›å¤ï¼ˆå¸¦ç”¨æˆ·ç”»åƒï¼‰
        agent_response = await self.llm_client.chat_with_context(
            query=message,
            context=context_string,
            history=history,
            user_profile=user_profile
        )
        
        # 8. å¼‚æ­¥æ›´æ–°ç”¨æˆ·ç”»åƒï¼ˆä¸é˜»å¡å“åº”ï¼‰
        if self.profile_service:
            asyncio.create_task(
                self.profile_service.update_from_message(user_id, message, domains)
            )
        
        # 9. ä¿å­˜Agentæ¶ˆæ¯åˆ°MySQL
        agent_msg_id = str(uuid4())
        agent_now = datetime.now(timezone.utc)
        await self.message_repo.create_message(
            message_id=agent_msg_id,
            session_id=session_id,
            role=MessageRole.AGENT,
            content=agent_response,
            context_string=context_string,
            context_data=context_data,
            created_at=agent_now
        )
        
        # 9. æ›´æ–°ä¼šè¯ç»Ÿè®¡
        await self.session_repo.update_stats(session_id)
        
        logger.info(
            f"Chat message processed: user_msg={user_msg_id}, "
            f"agent_msg={agent_msg_id}, session={session_id}"
        )
        
        # 10. è¿”å›å“åº”
        return {
            "user_message": {
                "message_id": user_msg_id,
                "role": "user",
                "content": message,
                "attached_papers": attached_papers,
                "created_at": now.isoformat() + "Z"
            },
            "agent_message": {
                "message_id": agent_msg_id,
                "role": "agent",
                "content": agent_response,
                "context_string": context_string,
                "context_data": context_data,
                "created_at": agent_now.isoformat() + "Z"
            },
            "status": {
                "graph_updated": True,
                "papers_parsed": attached_papers,
                "community_updated": True
            }
        }

    async def _generate_graph_context(
        self,
        user_id: str,
        query: str,
        domains: List[str]
    ) -> tuple:
        """
        ä»å›¾è°±ä¸­æ£€ç´¢ç”Ÿæˆcontext
        
        ä½¿ç”¨ç®€åŒ–çš„ group_id æ–¹æ¡ˆï¼š
        - å…¬å…±é¢†åŸŸå›¾è°±ï¼šdomain:{domain}ï¼ˆæ‰€æœ‰ç”¨æˆ·å…±äº«ï¼‰
        - ç”¨æˆ·ç§æœ‰ç¬”è®°ï¼šuser:{user_id}:notesï¼ˆå¯é€‰ï¼‰
        
        Args:
            user_id: ç”¨æˆ·ID
            query: ç”¨æˆ·æŸ¥è¯¢
            domains: ç ”ç©¶é¢†åŸŸåˆ—è¡¨ï¼ˆå¦‚ ["AI", "ML"]ï¼‰
            
        Returns:
            (context_string, context_data) å…ƒç»„
        """
        try:
            graphiti = await get_enhanced_graphiti()
            
            start_time = time.time()
            
            # æ ¹æ® domains æ„å»º group_idsï¼ˆå…¬å…±é¢†åŸŸ + ç”¨æˆ·ç¬”è®°ï¼‰
            group_ids = get_search_group_ids(
                user_id=user_id,
                domains=domains,
                include_user_notes=True
            )
            
            logger.info(
                f"ğŸ” Searching with group_ids: domains={domains} -> "
                f"group_ids={group_ids}"
            )
            
            # ä½¿ç”¨Graphitiçš„searchæ–¹æ³•ï¼Œä¼ å…¥ group_ids
            search_results = await graphiti.search(
                query=query,
                user_id=user_id,
                group_ids=group_ids,  # æŒ‰ domain çš„ group_ids è¿‡æ»¤
                limit=10
            )
            
            search_time_ms = int((time.time() - start_time) * 1000)
            
            # æ ¼å¼åŒ–context_string
            context_string = self._format_search_results_to_string(search_results, domains)
            
            # æ„å»ºcontext_data
            context_data = {
                "source": "graph",
                "domains_filtered": domains if domains else [],
                "group_ids_searched": group_ids,
                "search_results": [
                    {
                        "type": getattr(result, 'node_type', 'entity') if hasattr(result, 'node_type') else 'entity',
                        "uuid": getattr(result, 'uuid', str(uuid4())),
                        "name": getattr(result, 'name', 'Unknown'),
                        "snippet": str(getattr(result, 'fact', ''))[:200] if hasattr(result, 'fact') else '',
                        "relevance_score": getattr(result, 'score', 0.0) if hasattr(result, 'score') else 0.0,
                        "source": "Your research notes"
                    }
                    for result in search_results[:5]
                ] if search_results else [],
                "search_stats": {
                    "total_searched": len(search_results) if search_results else 0,
                    "total_returned": min(5, len(search_results)) if search_results else 0,
                    "search_time_ms": search_time_ms,
                    "group_ids_count": len(group_ids)
                }
            }
            
            return context_string, context_data
            
        except Exception as e:
            logger.error(f"Graph search failed: {e}")
            return "", {
                "source": "graph",
                "domains_filtered": domains if domains else [],
                "group_ids_searched": [],
                "search_results": [],
                "search_stats": {
                    "total_searched": 0,
                    "total_returned": 0,
                    "search_time_ms": 0,
                    "group_ids_count": 0
                }
            }

    async def _generate_paper_context(
        self,
        paper_ids: List[str],
        query: str,
        user_id: str
    ) -> tuple:
        """
        ä»è®ºæ–‡ä¸­ç”Ÿæˆcontext
        
        æµç¨‹ï¼š
        1. è·å–è®ºæ–‡è®°å½•
        2. å¦‚æœè®ºæ–‡æœªè§£æï¼Œåˆ™è§£æï¼ˆæ£€æŸ¥é‡å¤ï¼‰
        3. ä½¿ç”¨ LLM ä»è§£æå†…å®¹ä¸­æå–ä¸ç”¨æˆ·æ¶ˆæ¯ç›¸å…³çš„å†…å®¹
        4. å¼‚æ­¥å°†æ–°è§£æçš„è®ºæ–‡æ·»åŠ åˆ°å…¬å…±å›¾è°±
        
        Args:
            paper_ids: è®ºæ–‡IDåˆ—è¡¨
            query: ç”¨æˆ·æŸ¥è¯¢
            user_id: ç”¨æˆ·ID
            
        Returns:
            (context_string, context_data) å…ƒç»„
        """
        from app.services.pdf_parser import PDFParser
        from app.models.db_models import PaperStatus
        
        start_time = time.time()
        papers_to_add_to_graph = []  # æ–°è§£æçš„è®ºæ–‡ï¼Œéœ€è¦æ·»åŠ åˆ°å…¬å…±å›¾è°±
        
        try:
            # 1. è·å–è®ºæ–‡è®°å½•
            papers = await self.paper_repo.get_by_ids(paper_ids, user_id)
            
            if not papers:
                return "", {
                    "source": "paper",
                    "search_results": [],
                    "search_stats": {
                        "total_searched": 0,
                        "total_returned": 0,
                        "search_time_ms": 0
                    }
                }
            
            context_parts = []
            search_results = []
            pdf_parser = PDFParser()
            
            for paper in papers:
                try:
                    parsed_content = paper.parsed_content
                    was_newly_parsed = False
                    
                    # 2. å¦‚æœè®ºæ–‡æœªè§£æï¼Œåˆ™è§£æ
                    if not parsed_content or paper.status != PaperStatus.PARSED:
                        parsed_content = await self._parse_paper_with_dedup(
                            paper, pdf_parser
                        )
                        was_newly_parsed = True
                    elif isinstance(parsed_content, str):
                        parsed_content = json.loads(parsed_content)
                    
                    if not parsed_content:
                        logger.warning(f"No parsed content for paper: {paper.id}")
                        continue
                    
                    # æ ‡è®°æ–°è§£æçš„è®ºæ–‡ï¼Œç¨åæ·»åŠ åˆ°å…¬å…±å›¾è°±
                    if was_newly_parsed and not paper.added_to_graph:
                        papers_to_add_to_graph.append(paper.id)
                    
                    # 3. ä½¿ç”¨ LLM æå–ä¸æŸ¥è¯¢ç›¸å…³çš„å†…å®¹
                    relevant_content = await self._extract_relevant_content(
                        parsed_content, query, paper.filename
                    )
                    
                    if relevant_content:
                        context_parts.append(relevant_content)
                        
                        search_results.append({
                            "type": "paper",
                            "uuid": paper.id,
                            "name": paper.filename,
                            "title": parsed_content.get("title", paper.filename),
                            "snippet": relevant_content[:300],
                            "relevance_score": 1.0,
                            "source": f"Paper: {paper.filename}"
                        })
                        
                except Exception as e:
                    logger.error(f"Failed to process paper {paper.id}: {e}")
                    continue
            
            search_time_ms = int((time.time() - start_time) * 1000)
            
            if context_parts:
                context_string = "æ ¹æ®æ‚¨é™„å¸¦çš„è®ºæ–‡ï¼Œæ‰¾åˆ°ä»¥ä¸‹ç›¸å…³ä¿¡æ¯ï¼š\n\n" + "\n\n---\n\n".join(context_parts)
            else:
                context_string = ""
            
            context_data = {
                "source": "paper",
                "search_results": search_results,
                "search_stats": {
                    "total_searched": len(papers),
                    "total_returned": len(search_results),
                    "search_time_ms": search_time_ms
                }
            }
            
            # 4. å¼‚æ­¥å°†æ–°è§£æçš„è®ºæ–‡æ·»åŠ åˆ°å…¬å…±å›¾è°±ï¼ˆä¸é˜»å¡å“åº”ï¼‰
            if papers_to_add_to_graph:
                for paper_id in papers_to_add_to_graph:
                    asyncio.create_task(
                        self._add_paper_to_graph_async(paper_id, user_id)
                    )
            
            return context_string, context_data
            
        except Exception as e:
            logger.error(f"Paper context generation failed: {e}")
            return "", {
                "source": "paper",
                "search_results": [],
                "search_stats": {
                    "total_searched": 0,
                    "total_returned": 0,
                    "search_time_ms": 0
                }
            }

    async def _parse_paper_with_dedup(self, paper, pdf_parser) -> Optional[Dict]:
        """
        è§£æè®ºæ–‡å¹¶æ£€æŸ¥é‡å¤
        
        æµç¨‹ï¼š
        1. è§£æç¬¬ä¸€é¡µè·å–æ ‡é¢˜
        2. æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦å·²å­˜åœ¨ç›¸åŒæ ‡é¢˜çš„è®ºæ–‡
        3. å¦‚æœå­˜åœ¨ï¼Œä½¿ç”¨å·²æœ‰çš„è§£æç»“æœ
        4. å¦‚æœä¸å­˜åœ¨ï¼Œç»§ç»­è§£æå®Œæ•´è®ºæ–‡
        
        Args:
            paper: è®ºæ–‡è®°å½•
            pdf_parser: PDF è§£æå™¨
            
        Returns:
            è§£æåçš„å†…å®¹å­—å…¸
        """
        from app.models.db_models import PaperStatus
        import os
        
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not paper.file_path or not os.path.exists(paper.file_path):
                logger.error(f"Paper file not found: {paper.file_path}")
                return None
            
            # è¯»å–æ–‡ä»¶
            with open(paper.file_path, 'rb') as f:
                file_bytes = f.read()
            
            # è§£æè®ºæ–‡
            logger.info(f"Parsing paper: {paper.filename}")
            parsed_content = await pdf_parser.parse(file_bytes, paper.filename)
            
            if not parsed_content:
                return None
            
            title = parsed_content.get("title", "")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é‡å¤ï¼ˆé€šè¿‡æ ‡é¢˜ï¼‰
            if title:
                existing = await self.paper_repo.find_by_title(title)
                if existing and existing.id != paper.id and existing.parsed_content:
                    logger.info(f"Found duplicate paper by title: {title}")
                    # ä½¿ç”¨å·²æœ‰çš„è§£æç»“æœ
                    parsed_content = existing.parsed_content
                    if isinstance(parsed_content, str):
                        parsed_content = json.loads(parsed_content)
            
            # æ›´æ–°è®ºæ–‡è®°å½•
            paper.parsed_content = parsed_content
            paper.status = PaperStatus.PARSED
            await self.paper_repo.update(paper)
            
            return parsed_content
            
        except Exception as e:
            logger.error(f"Failed to parse paper: {e}")
            paper.status = PaperStatus.FAILED
            paper.parse_error = str(e)
            await self.paper_repo.update(paper)
            return None

    async def _extract_relevant_content(
        self,
        parsed_content: Dict,
        query: str,
        filename: str
    ) -> str:
        """
        ä½¿ç”¨ LLM ä»è®ºæ–‡å†…å®¹ä¸­æå–ä¸æŸ¥è¯¢ç›¸å…³çš„å†…å®¹
        
        Args:
            parsed_content: è§£æåçš„è®ºæ–‡å†…å®¹
            query: ç”¨æˆ·æŸ¥è¯¢
            filename: æ–‡ä»¶å
            
        Returns:
            ä¸æŸ¥è¯¢ç›¸å…³çš„å†…å®¹æ‘˜è¦
        """
        try:
            title = parsed_content.get("title", filename)
            abstract = parsed_content.get("abstract", "")
            sections = parsed_content.get("sections", [])
            
            # æ„å»ºè®ºæ–‡å†…å®¹æ‘˜è¦ï¼ˆé™åˆ¶é•¿åº¦ï¼‰
            paper_summary = f"æ ‡é¢˜: {title}\n\næ‘˜è¦: {abstract}\n\n"
            
            for section in sections[:6]:  # å–å‰6ä¸ªç« èŠ‚
                heading = section.get("heading", section.get("title", ""))
                content = section.get("content", "")[:800]
                paper_summary += f"## {heading}\n{content}\n\n"
            
            # é™åˆ¶æ€»é•¿åº¦
            if len(paper_summary) > 6000:
                paper_summary = paper_summary[:6000] + "..."
            
            # ä½¿ç”¨ LLM æå–ç›¸å…³å†…å®¹
            prompt = f"""è¯·ä»ä»¥ä¸‹è®ºæ–‡å†…å®¹ä¸­æå–ä¸ç”¨æˆ·é—®é¢˜æœ€ç›¸å…³çš„ä¿¡æ¯ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{query}

è®ºæ–‡å†…å®¹ï¼š
{paper_summary}

è¯·æå–å¹¶æ€»ç»“ä¸ç”¨æˆ·é—®é¢˜æœ€ç›¸å…³çš„å†…å®¹ï¼ˆä¸è¶…è¿‡800å­—ï¼‰ã€‚å¦‚æœè®ºæ–‡å†…å®¹ä¸é—®é¢˜ä¸å¤ªç›¸å…³ï¼Œè¯·ç®€è¦è¯´æ˜è®ºæ–‡çš„ä¸»è¦å†…å®¹ã€‚
åªè¿”å›æå–çš„å†…å®¹ï¼Œä¸è¦æ·»åŠ é¢å¤–è¯´æ˜ã€‚"""

            relevant_content = await self.llm_client.chat(
                messages=[{"role": "user", "content": prompt}],
                temperature=0.3,
                max_tokens=1500
            )
            
            return f"**{title}**\n\n{relevant_content}"
            
        except Exception as e:
            logger.error(f"Failed to extract relevant content: {e}")
            # é™çº§ï¼šè¿”å›æ‘˜è¦
            abstract = parsed_content.get("abstract", "")
            title = parsed_content.get("title", filename)
            return f"**{title}**\n\n{abstract[:500]}" if abstract else ""

    async def _add_paper_to_graph_async(
        self,
        paper_id: str,
        user_id: str
    ) -> None:
        """
        å¼‚æ­¥å°†è®ºæ–‡æ·»åŠ åˆ°å…¬å…±å›¾è°±ï¼ˆåå°ä»»åŠ¡ï¼‰
        
        è°ƒç”¨ IngestService.add_paper_to_graph å°†è®ºæ–‡æ·»åŠ åˆ°å…¬å…±å›¾è°±ã€‚
        
        Args:
            paper_id: è®ºæ–‡ID
            user_id: ç”¨æˆ·ID
        """
        try:
            from app.services.ingest_service import IngestService
            
            ingest_service = IngestService(paper_repo=self.paper_repo)
            
            result = await ingest_service.add_paper_to_graph(
                paper_id=paper_id,
                user_id=user_id
            )
            
            logger.info(
                f"âœ… Paper {paper_id} auto-added to public graph | "
                f"domains={result.get('domains')} | "
                f"episodes={result.get('episodes_added')}"
            )
            
        except Exception as e:
            logger.error(f"Failed to auto-add paper {paper_id} to graph: {e}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸ä¸»æµç¨‹ç»§ç»­

    def _format_search_results_to_string(
        self, 
        results, 
        domains: Optional[List[str]] = None
    ) -> str:
        """
        æ ¼å¼åŒ–æ£€ç´¢ç»“æœä¸ºcontextå­—ç¬¦ä¸²
        
        Args:
            results: æœç´¢ç»“æœåˆ—è¡¨
            domains: è¿‡æ»¤çš„ç ”ç©¶é¢†åŸŸåˆ—è¡¨
            
        Returns:
            æ ¼å¼åŒ–çš„contextå­—ç¬¦ä¸²
        """
        if not results:
            return ""
        
        # æ„å»ºå¼€å¤´ï¼Œè¯´æ˜æ£€ç´¢èŒƒå›´
        if domains:
            domain_str = ", ".join(domains)
            context = f"æ ¹æ®æ‚¨åœ¨ {domain_str} é¢†åŸŸçš„çŸ¥è¯†å›¾è°±æ£€ç´¢ï¼Œæ‰¾åˆ°ä»¥ä¸‹ç›¸å…³ä¿¡æ¯ï¼š\n\n"
        else:
            context = "æ ¹æ®æ‚¨çš„çŸ¥è¯†å›¾è°±æ£€ç´¢ï¼Œæ‰¾åˆ°ä»¥ä¸‹ç›¸å…³ä¿¡æ¯ï¼š\n\n"
        
        for i, result in enumerate(results[:5], 1):
            name = getattr(result, 'name', 'Unknown')
            node_type = getattr(result, 'node_type', 'entity') if hasattr(result, 'node_type') else 'entity'
            fact = str(getattr(result, 'fact', ''))[:200] if hasattr(result, 'fact') else ''
            source = getattr(result, 'source', 'Your research notes') if hasattr(result, 'source') else 'Your research notes'
            
            context += f"{i}. {name} ({node_type})\n"
            context += f"   {fact}...\n"
            context += f"   (æ¥æºï¼š{source})\n\n"
        
        return context

    async def get_history(
        self,
        session_id: str,
        user_id: str,
        limit: int = 50,
        offset: int = 0,
        order: str = "asc"
    ) -> Dict[str, Any]:
        """
        è·å–èŠå¤©å†å² - REQ-CHAT-4
        
        Args:
            session_id: ä¼šè¯ID
            user_id: ç”¨æˆ·ID
            limit: æ¯é¡µæ¶ˆæ¯æ•°
            offset: åç§»é‡
            order: æ’åºæ–¹å¼ (asc/desc)
            
        Returns:
            èŠå¤©å†å²æ•°æ®
        """
        # 1. éªŒè¯sessionå­˜åœ¨ä¸”å±äºè¯¥ç”¨æˆ·
        research_session = await self.session_repo.get_by_id_and_user(session_id, user_id)
        if not research_session:
            raise ValueError("SESSION_NOT_FOUND")
        
        # 2. æŸ¥è¯¢ä¼šè¯ä¿¡æ¯
        domains = SessionRepository.parse_domains(research_session.domains)
        
        session_info = {
            "title": research_session.title,
            "domains": domains,
            "created_at": research_session.created_at.isoformat() + "Z" if research_session.created_at else None
        }
        
        # 3. æŸ¥è¯¢æ¶ˆæ¯
        messages, total = await self.message_repo.get_by_session(
            session_id=session_id,
            limit=limit,
            offset=offset,
            order=order
        )
        
        # 4. æ ¼å¼åŒ–æ¶ˆæ¯
        message_list = [MessageRepository.format_message(msg) for msg in messages]
        
        return {
            "session_id": session_id,
            "session_info": session_info,
            "messages": message_list,
            "pagination": {
                "total": total,
                "limit": limit,
                "offset": offset,
                "has_more": (offset + limit) < total
            }
        }
